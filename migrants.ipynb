{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Missing Migrants Twitter Classification\n",
    "\n",
    "In this project we are building a classifier to help the [Missing Migrants project](https://missingmigrants.iom.int/). \n",
    "\n",
    "IOM needs to find news about migrants that die or go missing, all around the world. One of the sources is Twitter. They have several search terms that they can use (\"missing migrant\", \"migrant death\", \"migrant drown\", etc.), but the terms return many tweets that are not actual _reports_ of new migrants that have gone missing. \n",
    "\n",
    "Our job is to build a classifier that will get a set of tweets, and return a classification: 1 if it is a specific news report about a missing or dead migrant, 0 otherwise. \n",
    "\n",
    "We have a training set, ~2200 labelled tweets, on which to train the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "! pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, VectorizerMixin\n",
    "from utils import ngrammer, clean_html, get_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('max_colwidth', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from utils import clean_html\n",
    "from sklearn.feature_extraction.text import strip_accents_unicode\n",
    "\n",
    "def clean_twitter(s):\n",
    "    \"\"\" Cleans Twitter specific issues \n",
    "    \n",
    "    Can you think of what else you might need to add here?\n",
    "    \"\"\"\n",
    "    s = sub(r'@\\w+', '', s) #remove @ mentions from tweets    \n",
    "    return s\n",
    "\n",
    "def preprocessor(s):\n",
    "    \"\"\" For all basic string cleanup. \n",
    "    \n",
    "    Think of what you can add to this to improve things. What is\n",
    "    specific to your goal, how can you transform the text. Add tokens,\n",
    "    remove things, unify things. \n",
    "    \"\"\"\n",
    "    s = clean_html(s)\n",
    "    s = strip_accents_unicode(s.lower())\n",
    "    s = clean_twitter(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "def analyzer(s, ngram_range = (1,2)):\n",
    "    \"\"\" Does everything to turn raw documents into tokens.  \n",
    "    \n",
    "    Note: None of the above tokenizers are implemented!\n",
    "    \"\"\"\n",
    "    s = preprocessor(s)\n",
    "    pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "    unigrams = pattern.findall(s)\n",
    "    unigrams = [u for u in unigrams if u not in ENGLISH_STOP_WORDS]\n",
    "    tokens = ngrammer(unigrams, ngram_range)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('kaggle/train.csv').tweet\n",
    "y = pd.read_csv('kaggle/train.csv').label\n",
    "\n",
    "cutoff = 1750\n",
    "X_train, X_test, y_train, y_test = X[0:cutoff], X[cutoff:], y[0:cutoff], y[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import langdetect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "def detect(st):\n",
    "    try: \n",
    "        return langdetect.detect(st)\n",
    "    except LangDetectException:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('kaggle/train.csv')\n",
    "langs = df.tweet.map(detect)\n",
    "df[langs == 'en'].to_csv('kaggle/train-en.csv', quoting=csv.QUOTE_ALL, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Turn the text data into a fixed-length vectors that can be used to train a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# SKLearn's Logistic Regression allows us to use an L1 \n",
    "# penalty, turning it into a Lasso classifier.\n",
    "\n",
    "# What does the C parameter do, according to the documentation? \n",
    "lasso = LogisticRegression(penalty='l1', C=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Make your predictions for the test set (X_test/y_test)\n",
    "\n",
    "preds = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the mistakes\n",
    "\n",
    "false_pos, false_neg = get_errors(X_test, y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "name": "migrants.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
