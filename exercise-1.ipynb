{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "! pip install --quiet spacy\n",
    "! python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "rus = pd.read_csv('trolls/russia_201901_1_tweets_csv_hashed.csv')\n",
    "rus['tweet_time'] = rus.tweet_time.map(lambda d: pd.Timestamp(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def _split(df, year):\n",
    "    s = datetime(year=2000, month=7, day=1)\n",
    "    df = df[(df['tweet_time'] > s.replace(year=year)) & \n",
    "            (df['tweet_time'] < s.replace(year=year, month=11))]\n",
    "    return df.sample(750)\n",
    "\n",
    "def split_and_label(df, splits):\n",
    "    splits = [_split(df, year).assign(label = label) for year,label in splits]\n",
    "    return pd.concat(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = split_and_label(rus, [(2016, 'y2016'), (2017, 'y2017'), (2018, 'y2018')])\n",
    "X,y = df.tweet_text, df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'tweet': X, 'label': y}).to_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from re import sub, split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_embedding(V, y):\n",
    "    \"\"\" Visualizes a vocabulary embedding via TSNE \"\"\"\n",
    "    V = TruncatedSVD(50).fit_transform(V)\n",
    "    d = TSNE(metric='cosine').fit_transform(V)\n",
    "    d = pd.DataFrame(d).assign(label = y.reset_index(drop=True))\n",
    "    return sns.scatterplot(x = 0, y = 1, hue = 'label', data = d), d\n",
    "\n",
    "\n",
    "def clean_twitter(s):\n",
    "    \"\"\" Cleans Twitter specific issues\n",
    "    \n",
    "    Should probably clean out mentions, URLs, and RT's.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Use regular expressions to remove unwanted\n",
    "    # text and clean up our tweets to be more usable!\n",
    "\n",
    "    # BONUS: Try using the library \"spacy\" to \n",
    "    # do further processing, such as lemmatizing\n",
    "    # or replacing Named Entities with constants (i.e. \"[NAMED]\")\n",
    "    # or adding the part of speech or dependency code to the word \n",
    "\n",
    "    s = s.strip().lower()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Let's visualize our data by using nothing but the Sklearn default\n",
    "# cleaning and tokenizing\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "V = vectorizer.fit_transform(X)\n",
    "ax, d = plot_embedding(V, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Now let's see what our cleaning has done\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = clean_twitter)\n",
    "V = vectorizer.fit_transform(X)\n",
    "ax, d = plot_embedding(V, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Now try with TF-IDF vectorizer, and add implicit stopwords!\n",
    "# Can you get things to separate in the space in a better way? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "exercise-1.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
